#api/orchestrator.py
import logging
from typing import List, Dict, Any, Tuple, Optional
from .ai_services import async_llm_call, extract_xml
from .prompts import (
CONVERSATION_MANAGER_PROMPT,
SYLLABUS_GENERATOR_PROMPT,
PERSONA_ARCHITECT_PROMPT,
)
# logger = logging.getLogger(name) # Assuming 'name' should be __name__ or defined elsewhere
logger = logging.getLogger(__name__) # Corrected common practice

# State keys remain the same
STATE_EXPLANATION_START_INDEX = "explanation_start_index"
STATE_STAGE = "stage"
STATE_HISTORY = "conversation_history"
STATE_CURRENT_SYLLABUS = "current_syllabus_content"
STATE_FINAL_SYLLABUS = "final_syllabus_xml"
STATE_EXPLAINER_PROMPT = "explainer_system_prompt"
STATE_DISPLAY_SYLLABUS = "display_syllabus"
STATE_TRANSITION_EXPLAINER = "transition_to_explainer"

# Simplified Stages
STAGE_START = "START"
STAGE_NEGOTIATING = "NEGOTIATING"
# STAGE_FINALIZED_PENDING_PERSONA REMOVED
STAGE_EXPLAINING = "EXPLAINING"
STAGE_ERROR = "ERROR"

STATE_CURRENT_TITLE = "current_title" # Input: Title passed from view
STATE_GENERATED_TITLE = "generated_title" # Output: New title generated by orchestrator



# --- Constants for Title Generation ---
DEFAULT_CHAT_TITLE = "New Chat"
TITLE_GENERATION_THRESHOLD = 6
TITLE_GENERATION_MAX_MSGS = 6
TITLE_GENERATION_PROMPT = f"""
Generate a concise title for a chat session where a user is learning from an AI Tutor. You would be provided user chat history,
Analyze the provided chat history to identify the main learning topic or the user's primary educational goal.

The title should be short (ideally 3-6 words, maximum 17 words) and accurately reflect the subject matter.

Good examples capture the essence of the learning goal:
- Mastering Chess
- Controlling Brainwaves Intro
- Machine Learning Revision
- Quantum Computing Exploration
- Python for Data Science
- SQL Query Optimization Basics
- Learning Django Models

Output *only* the generated title text. Do not include any surrounding text, explanations, labels (like "Chat Title:"), or quotation marks.



"""

# # --- Helper function to generate chat title (Moved here) ---
# """Uses an LLM to generate a concise title based on the initial messages."""
#     if not history_snippet: return None
#     formatted_snippet = ""
#     for msg in history_snippet:
#         role = msg.get("role", "unknown")
#         # Filter internal messages if they exist in the snippet passed
#         msg_type = msg.get("type")
#         if msg_type == 'internal': continue
#         try:
#             content = ""
#             parts = msg.get("parts") # Assuming Gemini format primarily
#             if isinstance(parts, list) and parts:
#                 if isinstance(parts[0], dict): content = parts[0].get("text", "")
#                 elif isinstance(parts[0], str): content = parts[0]
#             elif isinstance(parts, str): content = parts
#             elif 'content' in msg: content = msg.get('content', '') # Fallback

#             if content: formatted_snippet += f"{role.capitalize()}: {content}\n"
#         except Exception as e: logger.warning(f"Could not format message for title gen: {e} - Msg: {msg}")

#     if not formatted_snippet.strip(): logger.warning("Formatted snippet for title gen is empty."); return None
async def generate_chat_title_async(history_snippet: List[Dict[str, Any]]) -> Optional[str]:
    

    prompt = TITLE_GENERATION_PROMPT
    logger.info("(Orchestrator) Requesting chat title generation...")
    try:
        title = await async_llm_call(prompt="",system_prompt=prompt,chat_history= history_snippet,temperature=0.4)
        if title and not title.startswith(("[E", "[B", "[EMPTY")):
            cleaned_title = title.strip().strip('"\'').strip()
            max_title_length = 150
            if len(cleaned_title) > max_title_length: cleaned_title = cleaned_title[:max_title_length] + "..."
            logger.info(f"(Orchestrator) Generated chat title: '{cleaned_title}'")
            return cleaned_title if cleaned_title else None
        else: logger.warning(f"(Orchestrator) LLM failed title gen. Response: {title}"); return None
    except Exception as e: logger.error(f"(Orchestrator) Error during title gen LLM call: {e}", exc_info=True); return None


# --- Helper Functions (get_intent_from_text, generate_*, get_last_syllabus - KEEP AS BEFORE) ---
# ... (Make sure get_last_syllabus_content_from_history exists if needed for recovery) ...
def get_intent_from_text(text: str) -> Optional[str]:
    # ... keep preferred implementation ...
    if not isinstance(text, str): return None
    stripped_text = text.strip()
    if "<request_syllabus_generation/>" in stripped_text: return "GENERATE"
    if "<request_syllabus_modification/>" in stripped_text: return "MODIFY"
    if "<request_finalization/>" in stripped_text: return "FINALIZE"
    if "<persona/>" in stripped_text: return "PERSONA"
    return "CONVERSE"

async def generate_syllabus_from_chat_async(conversation_history: List[Dict[str, Any]]) -> Tuple[str, str]:
    # ... keep implementation ...
    logger.info("Requesting syllabus gen/mod...")
    instruction = "Generate or modify the syllabus based on the provided conversation history according to your instructions"
    try:
        llm_response = await async_llm_call(prompt=instruction, chat_history=conversation_history, system_prompt=SYLLABUS_GENERATOR_PROMPT)
        # ... process response, return tuple ...
        if llm_response and not llm_response.startswith(("[E", "[B", "[EMPTY")):
            extracted = extract_xml(llm_response, "syllabus")
            if extracted: return extracted, ""
            elif llm_response.strip().startswith("<syllabus>"):
                inner = llm_response.strip()[len("<syllabus>"):-len("</syllabus>")].strip()
                if inner: return inner, ""
                else: return "", "[ERROR: Syllabus tags empty]"
            else: return "", f"[ERROR: Tags not found: {llm_response[:100]}...]"
        elif not llm_response or llm_response == "[EMPTY RESPONSE]": return "", "[ERROR: Empty response]"
        else: return "", llm_response # LLM Error
    except Exception as e: return "", f"[ERROR: Exception {e}]"

async def generate_explainer_prompt_async(conversation_history: List[Dict[str, Any]], final_syllabus_xml: str) -> Tuple[str, str]:
    # ... keep implementation ...
    logger.info("Requesting explainer prompt gen...")
    instruction = ""
    try:
        template = await async_llm_call(prompt=instruction, chat_history=conversation_history, system_prompt=PERSONA_ARCHITECT_PROMPT, temperature=0.6)
        print(template)
        # ... process template, replace placeholder, return tuple ...
        if template and not template.startswith(("[E", "[B", "[EMPTY")):
            if "{{SYLLABUS_SECTION}}" not in template: template += "\n\n---\n**Syllabus:**\n{{SYLLABUS_SECTION}}\n---"
            prompt = template.replace("{{SYLLABUS_SECTION}}", final_syllabus_xml)
            return prompt, ""
        elif not template or template == "[EMPTY RESPONSE]": return "", "[ERROR: Empty response]"
        else: return "", template # LLM Error
    except Exception as e: return "", f"[ERROR: Exception {e}]"

# Use code with caution.
# --- Main Orchestrator Logic (Simplified Flow) ---
# async def process_chat_message(
#     user_message: str, current_state: Dict[str, Any]
# ) -> Tuple[str, Dict[str, Any]]:
#     """
#     Processes user message. Handles negotiation, finalization, persona, and explanation.
#     No dedicated PENDING_PERSONA stage. Relies on view for current_syllabus on FINALIZE.
#     """
#     stage = current_state.get(STATE_STAGE, STAGE_START)
#     history: List[Dict[str, Any]] = current_state.get(STATE_HISTORY, [])
#     current_syllabus = current_state.get(STATE_CURRENT_SYLLABUS, None) # Still needed for FINALIZE check
#     logger.debug(f"Orchestrator received state: Stage={stage}, Has current_syllabus={current_syllabus is not None}")
#     ai_reply_for_user = ""
#     new_state = current_state.copy()
#     new_state.pop(STATE_DISPLAY_SYLLABUS, None)
#     new_state.pop(STATE_TRANSITION_EXPLAINER, None)

#     try:
#         # --- Negotiation & Post-Finalization Handling ---
#         # Combines original NEGOTIATING and PENDING_PERSONA logic
#         if stage in [STAGE_START, STAGE_NEGOTIATING]: # Remains in NEGOTIATING until EXPLAINING starts
#             logger.info(f"Orchestrator: Stage={stage}. Calling Manager.")
#             # Set stage to NEGOTIATING if starting
#             if stage == STAGE_START:
#                  new_state[STATE_STAGE] = STAGE_NEGOTIATING

#             manager_response_text = await async_llm_call(prompt=user_message, chat_history=history, system_prompt=CONVERSATION_MANAGER_PROMPT,temperature=1)
#             manager_reply_to_add = manager_response_text
#             # Handle errors/empty before adding to history
#             if not manager_response_text or manager_response_text.startswith(("[ERROR", "[BLOCKED", "[EMPTY")):
#                  if not manager_response_text or manager_response_text == "[EMPTY RESPONSE]": manager_reply_to_add = "(Manager response was empty)"
#                  else: manager_reply_to_add = manager_response_text
#                  ai_reply_for_user = manager_reply_to_add # Set reply if manager failed
#             history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Append user message first
#             history.append({'role': 'model', 'parts': [{'text': manager_reply_to_add}]})

#             # Only process intent if manager call was successful
#             if manager_response_text and not manager_response_text.startswith(("[ERROR", "[BLOCKED", "[EMPTY")):
#                 intent = get_intent_from_text(manager_response_text)
#                 logger.info(f"Orchestrator: Manager Intent='{intent}' in Stage='{new_state[STATE_STAGE]}'") # Log current stage

#                 if intent in ["GENERATE", "MODIFY"]:
#                     # ... (Syllabus generation logic - same as before) ...
#                     syllabus_content, error_msg = await generate_syllabus_from_chat_async(history)
#                     if not error_msg:
#                         new_state[STATE_CURRENT_SYLLABUS] = syllabus_content
#                         new_state[STATE_DISPLAY_SYLLABUS] = f"<syllabus>\n{syllabus_content}\n</syllabus>"
#                         logger.info("Syllabus generated/modified.")
#                         follow_up_prompt = "The syllabus has been presented. Ask the user for feedback."
#                         feedback_request = await async_llm_call(prompt=follow_up_prompt, chat_history=history, system_prompt=CONVERSATION_MANAGER_PROMPT,temperature =1)
#                         feedback_reply_to_add = feedback_request #if feedback_request else "Here is the syllabus draft. What do you think?"
#                         history.append({'role': 'model', 'parts': [{'text': feedback_reply_to_add}]})
#                         ai_reply_for_user = feedback_reply_to_add
#                     else:
#                          logger.error(f"Syllabus action failed: {error_msg}")
#                          ai_reply_for_user = f"Sorry, error processing syllabus: {error_msg}"
#                          history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})

#                 elif intent == "FINALIZE":
#                     logger.debug(f"Checking FINALIZE. current_syllabus is set: {current_syllabus is not None}")
#                     # if not current_syllabus:
#                     #     current_syllabus = syllabus_content # Get from state if available
#                     if current_syllabus: # Check if there's a syllabus *in the current state* to finalize
#                         logger.info("Finalization successful. Asking for persona.")
#                         # Set final syllabus state *but stay in NEGOTIATING stage*
#                         new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{current_syllabus}\n</syllabus>"
#                         new_state.pop(STATE_CURRENT_SYLLABUS, None) # Clear transient syllabus

#                         # Ask the persona question
#                         follow_up_prompt = "The syllabus has been finalized. Ask the user about their preferred learning style..."
#                         persona_query = await async_llm_call(prompt=follow_up_prompt, chat_history=history, system_prompt=CONVERSATION_MANAGER_PROMPT,temperature=1)
#                         persona_reply_to_add = persona_query #if persona_query else "Great, syllabus finalized! How do you like to learn?"
#                         history.append({'role': 'model', 'parts': [{'text': persona_reply_to_add}]})
#                         ai_reply_for_user = persona_reply_to_add
#                     else:
#                         # Attempt to finalize but no current syllabus exists (e.g., first message is finalize)
#                         # Try to generate one based on history just in case
#                         logger.warning("FINALIZE intent but no current_syllabus in state. Attempting generation.")
#                         syllabus_content, error_msg = await generate_syllabus_from_chat_async(history)
#                         if not error_msg and syllabus_content:
#                             logger.info("Generated syllabus during finalize request.")
#                             new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{syllabus_content}\n</syllabus>" # Directly finalize
#                             new_state.pop(STATE_CURRENT_SYLLABUS, None)
#                             follow_up_prompt = "I've generated and finalized a syllabus based on our chat. Now, tell me about your preferred learning style..."
#                             persona_query = await async_llm_call(prompt=follow_up_prompt, chat_history=history, system_prompt=CONVERSATION_MANAGER_PROMPT, temperature=1)
#                             persona_reply_to_add = persona_query
#                             history.append({'role': 'model', 'parts': [{'text': persona_reply_to_add}]})
#                             ai_reply_for_user = persona_reply_to_add
#                             # Display the syllabus generated during finalize
#                             new_state[STATE_DISPLAY_SYLLABUS] = new_state[STATE_FINAL_SYLLABUS]
#                         else:
#                             logger.error(f"FINALIZE failed: Cannot finalize without a syllabus draft. Generation failed: {error_msg}")
#                             ai_reply_for_user = "It seems we haven't settled on a syllabus draft yet, and I couldn't create one now. Could you describe what you want to learn first?"
#                             history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
#                             # Stay in NEGOTIATING


#                     # else:
#                     #     if syllabus_content:
#                     #     print("Error is here")
#                     #     # STAGE REMAINS NEGOTIATING - waiting for persona input
#                     #                     else:
#                     #     logger.warning("FINALIZE intent but no current_syllabus found in state.")
#                     #     #ai_reply_for_user = "It seems we haven't settled on a syllabus draft yet..."
#                     #    # history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
#                     #     # Stay in NEGOTIATING

#                 elif intent == "PERSONA":
#                      logger.info(f"Orchestrator: PERSONA intent detected during {new_state[STATE_STAGE]} stage.")
#                      # Check if syllabus was finalized (indicated by STATE_FINAL_SYLLABUS being set)
#                      final_syllabus_in_state = new_state.get(STATE_FINAL_SYLLABUS)
#                      # if not final_syllabus_in_state:
#                      #     # If no final syllabus, but there is a *current* one, finalize it now
#                      #     current_syllabus_to_finalize = new_state.get(STATE_CURRENT_SYLLABUS)
#                      #     if current_syllabus_to_finalize:
#                      #        logger.info("PERSONA intent received with current syllabus; finalizing now.")
#                      #        new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{current_syllabus_to_finalize}\n</syllabus>"
#                      #        new_state.pop(STATE_CURRENT_SYLLABUS, None)
#                      #        final_syllabus_in_state = new_state.get(STATE_FINAL_SYLLABUS)
#                      #     else:
#                      #        # Try generating if no syllabus exists at all
#                      #        logger.warning("PERSONA intent but no syllabus found. Attempting generation first.")
#                      #        syllabus_content, error_msg = await generate_syllabus_from_chat_async(history)
#                      #        if not error_msg and syllabus_content:
#                      #            logger.info("Generated and finalized syllabus during persona request.")
#                      #            new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{syllabus_content}\n</syllabus>"
#                      #            final_syllabus_in_state = new_state.get(STATE_FINAL_SYLLABUS)
#                      #            new_state[STATE_DISPLAY_SYLLABUS] = new_state[STATE_FINAL_SYLLABUS] # Display it
#                      #        else:
#                      #             logger.error(f"PERSONA failed: Cannot set persona without syllabus. Generation failed: {error_msg}")
#                      #             ai_reply_for_user = "I understand your learning preference, but we need a syllabus first, and I couldn't create one. Could you describe what you want to learn?"
#                      #             history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
#                      #             final_syllabus_in_state = None # Ensure we don't proceed

#                      if not final_syllabus_in_state:
#                          # Check if there's a current syllabus to finalize implicitly
#                          current_syllabus_to_finalize = new_state.get(STATE_CURRENT_SYLLABUS)
#                          if current_syllabus_to_finalize:
#                              logger.info("PERSONA intent received with current syllabus; finalizing now.")
#                              new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{current_syllabus_to_finalize}\n</syllabus>"
#                              new_state.pop(STATE_CURRENT_SYLLABUS, None)
#                              final_syllabus_in_state = new_state[STATE_FINAL_SYLLABUS]
#                          else:
#                              # Try generating if no syllabus exists at all
#                              logger.warning("PERSONA intent but no syllabus found. Attempting generation first.")
#                              syllabus_content, error_msg = await generate_syllabus_from_chat_async(history)
#                              if not error_msg and syllabus_content:
#                                  logger.info("Generated and finalized syllabus during persona request.")
#                                  new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{syllabus_content}\n</syllabus>"
#                                  final_syllabus_in_state = new_state[STATE_FINAL_SYLLABUS]
#                                  new_state[STATE_DISPLAY_SYLLABUS] = new_state[STATE_FINAL_SYLLABUS] # Display it
#                              else:
#                                  logger.error(f"PERSONA failed: Cannot set persona without syllabus. Generation failed: {error_msg}")
#                                  ai_reply_for_user = "I understand your learning preference, but we need a syllabus first, and I couldn't create one now. Could you describe what you want to learn?"
#                                  # Append response only if we are not proceeding to explainer generation
#                                  if not final_syllabus_in_state:
#                                      history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
#                                  # Ensure we don't proceed if syllabus creation failed
#                                  # No 'else' needed here, final_syllabus_in_state check below handles it

#                      if final_syllabus_in_state:
#                          logger.info("Syllabus previously finalized or finalized now. Generating explainer prompt.")
#                          # Generate the explainer prompt
#                          gen_explainer_prompt, error_msg = await generate_explainer_prompt_async(history, final_syllabus_in_state)
#                          if not error_msg and gen_explainer_prompt: # Success
#                              new_state[STATE_EXPLAINER_PROMPT] = gen_explainer_prompt
#                              new_state[STATE_STAGE] = STAGE_EXPLAINING # <<< TRANSITION TO EXPLAINING
#                              new_state[STATE_TRANSITION_EXPLAINER] = True
#                              logger.info("Orchestrator: Explainer prompt generated. Moving to EXPLAINING stage.")
#                              explainer_intro = await async_llm_call(prompt="Introduce yourself briefly and confirm you are ready to start based on the finalized syllabus.", chat_history=history, system_prompt=gen_explainer_prompt)
#                              explainer_intro_to_add = explainer_intro if explainer_intro else "Right, let's get started!"
#                              history.append({'role': 'model', 'parts': [{'text': explainer_intro_to_add}]})
#                              ai_reply_for_user = explainer_intro_to_add
#                          else: # Error generating prompt
#                              logger.error(f"Failed to generate explainer prompt: {error_msg}")
#                              ai_reply_for_user = f"Sorry, error setting up the learning session: {error_msg or '[No Prompt]'}"
#                              history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
#                              new_state[STATE_STAGE] = STAGE_ERROR
#                      # Removed the 'else' block that previously handled "persona before finalize"
#                      # because the logic above now attempts to finalize or generate if needed.
#                      # If final_syllabus_in_state is still None/False after attempts,
#                      # ai_reply_for_user would have been set with an error message already.

#                 elif intent == "CONVERSE":
#                     ai_reply_for_user = manager_reply_to_add # Use the text added to history

#             # else case: Manager call failed, ai_reply_for_user already set in the error handling block above

#         # --- Explanation Phase ---
#         # This block is only reached if stage was explicitly set to EXPLAINING above
#         elif stage == STAGE_EXPLAINING:
#             # (Keep logic as is)
#             logger.info(f"Orchestrator: Stage={stage}. Calling Explainer.")
#             # ... (check prompt, call explainer, add to history, set ai_reply) ...
#             explainer_prompt_in_state = new_state.get(STATE_EXPLAINER_PROMPT)
#             if not explainer_prompt_in_state: #... error handling ...
#                 ai_reply_for_user = "[SYSTEM ERROR: Explainer setup incomplete.]"
#                 history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Append user message
#                 history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
#                 new_state[STATE_STAGE] = STAGE_ERROR
#             else:
#                 history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Append user message first
#                 explainer_response = await async_llm_call(prompt=user_message, chat_history=history, system_prompt=explainer_prompt_in_state)
#                 explainer_reply_to_add = explainer_response
#                 if explainer_response and not explainer_response.startswith(("[E", "[B", "[EMPTY")): ai_reply_for_user = explainer_response
#                 elif not explainer_response or explainer_response == "[EMPTY RESPONSE]": explainer_reply_to_add = "(No response generated.)"; ai_reply_for_user = explainer_reply_to_add
#                 else: explainer_reply_to_add = f"Issue generating explanation: {explainer_response}"; ai_reply_for_user = explainer_reply_to_add
#                 history.append({'role': 'model', 'parts': [{'text': explainer_reply_to_add}]})

#         # --- Error Stage / Unknown Stage / Exception Handling ---
#         elif stage == STAGE_ERROR: # ... as before ...
#              logger.warning("Orchestrator: Currently in ERROR stage.")
#              ai_reply_for_user = "I'm sorry, an internal error occurred. Please try starting a new conversation."
#              # Optionally clear history or revert state here if desired upon error
#              # history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Log user message even in error
#              # history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]}) # Avoid infinite error loops?
#         else: # ... as before ...
#              logger.error(f"Orchestrator: Unknown stage: {stage}")
#              ai_reply_for_user = "[SYSTEM ERROR: Invalid application state.]"
#              history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Append user message
#              history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
#              new_state[STATE_STAGE] = STAGE_ERROR

#     except Exception as e:
#         # ... (exception handling as before) ...
#         logger.error(f"Orchestrator: Unhandled exception: {e}", exc_info=True)
#         ai_reply_for_user = "[SYSTEM ERROR: Unhandled Exception]"
#         new_state[STATE_STAGE] = STAGE_ERROR
#         try:
#             # Try to record the user message and error response
#             if not history or history[-1].get('role') != 'model' or history[-1]['parts'][0]['text'] != ai_reply_for_user:
#                  history.append({'role': 'user', 'parts': [{'text': user_message}]})
#                  history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
#         except Exception as log_e:
#             logger.error(f"Orchestrator: Failed to append error to history: {log_e}")


#     # --- Final State Update ---
#     new_state[STATE_HISTORY] = history
#     logger.debug(f"Orchestrator returning state: Stage={new_state.get(STATE_STAGE)}, History Length={len(new_state.get(STATE_HISTORY, []))}, Display Syllabus={new_state.get(STATE_DISPLAY_SYLLABUS) is not None}, Transition Explainer={new_state.get(STATE_TRANSITION_EXPLAINER)}")
#     return ai_reply_for_user, new_state

# api/orchestrator.py (or could be in a separate helpers file)
import logging
from typing import List, Dict, Any, Optional
# Make sure extract_xml is available, either defined here or imported
from .ai_services import extract_xml # Assuming it's in ai_services.py

logger = logging.getLogger(__name__)

def get_last_syllabus_content_from_history(history: List[Dict[str, Any]]) -> Optional[str]:
    """
    Searches conversation history backwards for the most recent message containing
    valid syllabus content, typically indicated by <syllabus> tags within a
    'model' or 'system' message.

    Args:
        history: The conversation history list, where each item is a dictionary
                 expected to have 'role' and 'parts' keys. 'parts' is expected
                 to be a list, usually containing one dictionary with a 'text' key.

    Returns:
        The extracted inner text content of the last found <syllabus> tag,
        or None if no valid syllabus content is found in recent history.
    """
    logger.debug("Helper: Searching history backwards for last syllabus content...")
    if not history: # Handle empty history case
        logger.warning("Helper: History is empty, cannot find syllabus.")
        return None

    for i in range(len(history) - 1, -1, -1): # Iterate backwards from the last message
        message = history[i]
        msg_role = message.get('role')
        logger.debug(f"Helper: Checking history index {i}, Role: {msg_role}")

        # Check roles most likely to contain generated syllabus or system markers for it
        if msg_role in ['model', 'system']:
            parts_list = message.get('parts', [])
            content = ""

            # --- Robust Content Extraction from 'parts' ---
            # Handles cases where parts is a list of dicts, list of strings, or just a string
            if isinstance(parts_list, list) and len(parts_list) > 0:
                first_part = parts_list[0]
                if isinstance(first_part, dict):
                    content = first_part.get('text', '') # Get text if part is dict
                elif isinstance(first_part, str):
                    content = first_part # Use string directly if part is string
            elif isinstance(parts_list, str): # Handle if 'parts' itself was saved as a string
                 content = parts_list
            # --- End Robust Content Extraction ---

            if not content:
                 logger.debug(f"Helper: No content found in parts for message at index {i}")
                 continue # Skip messages with no content

            logger.debug(f"Helper: Content at index {i} starts with: '{content[:70]}...'")

            # --- Check 1: Extract content within <syllabus> tags ---
            # Use the dedicated function first for robustness (case-insensitive)
            extracted = extract_xml(content, "syllabus")
            if extracted:
                logger.info(f"Helper: FOUND syllabus via extract_xml at index {i}")
                return extracted # Return the successfully extracted content

            # --- Check 2: Handle WARN message from generator ---
            # If extract_xml failed, check if it's the warning message containing raw content
            if content.startswith("[WARN: Syllabus tags not found"):
                 raw_start_phrase = "raw response: "
                 raw_start_index = content.find(raw_start_phrase)
                 if raw_start_index != -1:
                     # Extract the raw content part
                     raw_content = content[raw_start_index + len(raw_start_phrase):].strip()
                     # Basic sanity check to see if it looks like syllabus content
                     if raw_content and (raw_content.startswith("Phase") or raw_content.startswith("Topic:")):
                          logger.info(f"Helper: FOUND raw syllabus content from warning message at index {i}")
                          return raw_content # Return the raw content identified in the warning
                     else:
                          logger.debug(f"Helper: Found WARN message at index {i}, but raw content didn't pass sanity check.")

            # If neither extraction nor warning matched, continue to the previous message
            logger.debug(f"Helper: No syllabus match found for content at index {i}")

    # If loop finishes without finding anything
    logger.warning("Helper: Finished searching history, did not find valid syllabus content.")
    return None
async def process_chat_message(
    user_message: str, current_state: Dict[str, Any]
) -> Tuple[str, Dict[str, Any]]:
    """
    Processes user message. Adds generated syllabus to history.
    Uses history lookup for finalization. Uses dedicated PENDING_PERSONA stage.
    """
    stage = current_state.get(STATE_STAGE, STAGE_START)
    history: List[Dict[str, Any]] = current_state.get(STATE_HISTORY, [])
    # We no longer need current_syllabus passed in, will read from history
    current_title = current_state.get(STATE_CURRENT_TITLE, DEFAULT_CHAT_TITLE)
    logger.debug(f"Orchestrator received state: Stage={stage}, Current Title='{current_title}'")

    # logger.debug(f"Orchestrator received state: Stage={stage}")

    ai_reply_for_user = ""
    new_state = current_state.copy() # Preserve incoming state keys like FINAL_SYLLABUS etc.
    # Clear transient flags for this turn
    new_state.pop(STATE_DISPLAY_SYLLABUS, None) # We don't use this flag anymore
    new_state.pop(STATE_TRANSITION_EXPLAINER, None)
    new_state.pop(STATE_CURRENT_SYLLABUS, None) # Clear just in case
    new_state.pop(STATE_GENERATED_TITLE, None)

    # User message already added to history by the VIEW before this call.

    try:
        # --- Negotiation Phase ---
        if stage in [STAGE_START, STAGE_NEGOTIATING]:
            logger.info(f"Orchestrator: Stage={stage}. Calling Manager.")
            if stage == STAGE_START: new_state[STATE_STAGE] = STAGE_NEGOTIATING

            manager_response_text = await async_llm_call(prompt=user_message, chat_history=history, system_prompt=CONVERSATION_MANAGER_PROMPT)
            manager_reply_to_add = manager_response_text
            # Handle errors/empty manager response
            if not manager_response_text or manager_response_text.startswith(("[E", "[B", "[EMPTY")):
                if not manager_response_text or manager_response_text == "[EMPTY RESPONSE]": manager_reply_to_add = "(Manager response was empty)"
                else: manager_reply_to_add = manager_response_text
                ai_reply_for_user = manager_reply_to_add
            history.append({'role': 'model', 'parts': [{'text': manager_reply_to_add}]})

            # Only process intent if manager call was successful
            if manager_response_text and not manager_response_text.startswith(("[E","[B","[EMPTY")):
                intent = get_intent_from_text(manager_response_text)
                logger.info(f"Orchestrator: Manager Intent='{intent}' in Stage='{new_state[STATE_STAGE]}'")

                if intent in ["GENERATE", "MODIFY"]:

                    syllabus_content, error_msg = await generate_syllabus_from_chat_async(history)
                    if error_msg:
                        #Try again
                        syllabus_content, error_msg = await generate_syllabus_from_chat_async(history)
                    print(syllabus_content)
                    if not error_msg: # Success generating content
                        logger.info("Syllabus generated/modified successfully.")
                        syllabus_full_tagged_content = f"<syllabus>\n{syllabus_content}\n</syllabus>"
                        new_state[STATE_DISPLAY_SYLLABUS] = syllabus_full_tagged_content
                        # --- ADD SYLLABUS TO HISTORY ---
                        # Add as 'system' role, view's save_message will set message_type='syllabus'
                        history.append({
                            'role': 'model',
                            'parts': [{'text': syllabus_full_tagged_content}]
                        })
                        logger.debug("Added system message with syllabus content to history.")
                        # --- END HISTORY ADDITION ---

                        # Ask for feedback
                        follow_up_prompt = "The syllabus has been presented. Ask the user for feedback."
                        feedback_request = await async_llm_call(prompt=follow_up_prompt, chat_history=history, system_prompt=CONVERSATION_MANAGER_PROMPT)
                        feedback_reply_to_add = feedback_request #if feedback_request else "Here is the syllabus draft. How does it look?"
                        history.append({'role': 'model', 'parts': [{'text': feedback_reply_to_add}]})
                        ai_reply_for_user = feedback_reply_to_add
                    else: # Error generating syllabus
                        logger.error(f"Syllabus action failed: {error_msg}")
                        ai_reply_for_user = f"Sorry, error processing syllabus: {error_msg}"
                        history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})

                elif intent == "FINALIZE":
                    # --- USE HISTORY LOOKUP ---
                    last_syllabus_content = get_last_syllabus_content_from_history(history)
                    logger.debug(f"Checking FINALIZE. Found syllabus in history: {last_syllabus_content is not None}")

                    if last_syllabus_content:
                        logger.info("Finalization successful based on history. Asking for persona.")
                        # Set final syllabus state & CHANGE Stage
                        new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{last_syllabus_content}\n</syllabus>"
                        
                        new_state.pop(STATE_CURRENT_SYLLABUS, None) # Clear if exists

                        # Ask the persona question
                        follow_up_prompt = "The syllabus has been finalized. Ask the user about their preferred learning style..."
                        persona_query = await async_llm_call(prompt=follow_up_prompt, chat_history=history, system_prompt=CONVERSATION_MANAGER_PROMPT)
                        persona_reply_to_add = persona_query #if persona_query #else "Great, syllabus finalized! How do you like to learn?"
                        history.append({'role': 'model', 'parts': [{'text': persona_reply_to_add}]})
                        ai_reply_for_user = persona_reply_to_add
                    else: # Finalize requested but no syllabus found in history
                        logger.warning("FINALIZE intent but no syllabus found in history.")
                        # ai_reply_for_user = "It seems we haven't settled on a syllabus draft yet..."
                        # history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
                        # Stay in NEGOTIATING

                elif intent == "PERSONA":
                    logger.info(f"Orchestrator: PERSONA intent detected during {new_state[STATE_STAGE]} stage.")
                    # Check if syllabus was finalized (indicated by STATE_FINAL_SYLLABUS being set)
                    final_syllabus_in_state = new_state.get(STATE_FINAL_SYLLABUS)
                    if not final_syllabus_in_state:
                        last_syllabus_content = get_last_syllabus_content_from_history(history)
                        current_syllabus_to_finalize =  last_syllabus_content
                        if current_syllabus_to_finalize:
                            logger.info("PERSONA intent received with current syllabus; finalizing now.")
                            new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{current_syllabus_to_finalize}\n</syllabus>"
                            new_state.pop(STATE_CURRENT_SYLLABUS, None)
                            final_syllabus_in_state = new_state[STATE_FINAL_SYLLABUS]
                        else:
                            # Try generating if no syllabus exists at all
                            logger.warning("PERSONA intent but no syllabus found. Attempting generation first.")
                            syllabus_content, error_msg = await generate_syllabus_from_chat_async(history)
                            if not error_msg and syllabus_content:
                                logger.info("Generated and finalized syllabus during persona request.")
                                new_state[STATE_FINAL_SYLLABUS] = f"<syllabus>\n{syllabus_content}\n</syllabus>"
                                final_syllabus_in_state = new_state[STATE_FINAL_SYLLABUS]
                                new_state[STATE_DISPLAY_SYLLABUS] = new_state[STATE_FINAL_SYLLABUS] # Display it
                            else:
                                logger.error(f"PERSONA failed: Cannot set persona without syllabus. Generation failed: {error_msg}")
                                ai_reply_for_user = "I understand your learning preference, but we need a syllabus first, and I couldn't create one now. Could you describe what you want to learn?"
                                # Append response only if we are not proceeding to explainer generation
                                if not final_syllabus_in_state:
                                    history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
                                # Ensure we don't proceed if syllabus creation failed
                                # No 'else' needed here, final_syllabus_in_state check below handles it

                    if final_syllabus_in_state:
                        logger.info("Syllabus previously finalized or finalized now. Generating explainer prompt.")
                        # Generate the explainer prompt
                        gen_explainer_prompt, error_msg = await generate_explainer_prompt_async(history, final_syllabus_in_state)
                        print(gen_explainer_prompt)
                        if not error_msg and gen_explainer_prompt: # Success
                            new_state[STATE_EXPLAINER_PROMPT] = gen_explainer_prompt
                            new_state[STATE_STAGE] = STAGE_EXPLAINING # <<< TRANSITION TO EXPLAINING
                            new_state[STATE_TRANSITION_EXPLAINER] = True
                            logger.info("Orchestrator: Explainer prompt generated. Moving to EXPLAINING stage.")
                            explainer_intro = await async_llm_call(prompt="Introduce yourself briefly and confirm you are ready to start based on the finalized syllabus.", chat_history=[], system_prompt=gen_explainer_prompt)
                            explainer_intro_to_add = explainer_intro if explainer_intro else "Right, let's get started!"
                            start_index = len(history)
                            new_state[STATE_EXPLANATION_START_INDEX] = start_index

                            
                            history.append({'role': 'model', 'parts': [{'text': explainer_intro_to_add}]})
                            ai_reply_for_user = explainer_intro_to_add
                        else: # Error generating prompt
                            logger.error(f"Failed to generate explainer prompt: {error_msg}")
                            ai_reply_for_user = f"Sorry, error setting up the learning session: {error_msg or '[No Prompt]'}"
                            history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
                            new_state[STATE_STAGE] = STAGE_ERROR
                     # Removed the 'else' block that previously handled "persona before finalize"
                     # because the logic above now attempts to finalize or generate if needed.
                     # If final_syllabus_in_state is still None/False after attempts,
                     # ai_reply_for_user would have been set with an error message already.

                elif intent == "CONVERSE":
                    ai_reply_for_user = manager_reply_to_add # Use the text added to history

            # else case: Manager call failed, ai_reply_for_user already set in the error handling block above

        # --- Explanation Phase ---
        # This block is only reached if stage was explicitly set to EXPLAINING above
        elif stage == STAGE_EXPLAINING:
            # (Keep logic as is)
            logger.info(f"Orchestrator: Stage={stage}. Calling Explainer.")
            # ... (check prompt, call explainer, add to history, set ai_reply) ...
            explainer_prompt_in_state = new_state.get(STATE_EXPLAINER_PROMPT)
            start_idx = new_state.get(STATE_EXPLANATION_START_INDEX)
            print(explainer_prompt_in_state)
            print(len(history) - start_idx)
            if not explainer_prompt_in_state: #... error handling ...
                ai_reply_for_user = "[SYSTEM ERROR: Explainer setup incomplete.]"
                history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Append user message
                history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
                new_state[STATE_STAGE] = STAGE_ERROR
                # start_index = len(history) # If you are retrying things
                # new_state[STATE_EXPLANATION_START_INDEX] = start_index
            else:
                history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Append user message first
                explainer_response = await async_llm_call(prompt=user_message, chat_history=history[start_idx:], system_prompt=explainer_prompt_in_state)
                explainer_reply_to_add = explainer_response
                if explainer_response and not explainer_response.startswith(("[E", "[B", "[EMPTY")): ai_reply_for_user = explainer_response
                elif not explainer_response or explainer_response == "[EMPTY RESPONSE]": explainer_reply_to_add = "(No response generated.)"; ai_reply_for_user = explainer_reply_to_add
                else: explainer_reply_to_add = f"Issue generating explanation: {explainer_response}"; ai_reply_for_user = explainer_reply_to_add
                history.append({'role': 'model', 'parts': [{'text': explainer_reply_to_add}]})

        # --- Error Stage / Unknown Stage / Exception Handling ---
        elif stage == STAGE_ERROR: # ... as before ...
            logger.warning("Orchestrator: Currently in ERROR stage.")
            ai_reply_for_user = "I'm sorry, an internal error occurred. Please try starting a new conversation."
             # Optionally clear history or revert state here if desired upon error
             # history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Log user message even in error
             # history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]}) # Avoid infinite error loops?
        else: # ... as before ...
            logger.error(f"Orchestrator: Unknown stage: {stage}")
            ai_reply_for_user = "[SYSTEM ERROR: Invalid application state.]"
            history.append({'role': 'user', 'parts': [{'text': user_message}]}) # Append user message
            history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
            new_state[STATE_STAGE] = STAGE_ERROR
        final_message_count_this_turn = len(history)
        logger.debug(f"(Orchestrator) Checking title generation: Current Title='{current_title}', Msg Count={final_message_count_this_turn}, Threshold={TITLE_GENERATION_THRESHOLD}")

        if current_title == "New Chat" and final_message_count_this_turn >= TITLE_GENERATION_THRESHOLD:
            logger.info(f"(Orchestrator) Conditions met for title generation.")
            # Get snippet from the *current* history list in memory
            # Ensure correct format for helper function (role, parts/content)
            history_snippet_for_title = history

            generated_title = await generate_chat_title_async(history_snippet_for_title)
            if generated_title:
                # Add generated title to the state dictionary to be returned
                new_state[STATE_GENERATED_TITLE] = generated_title
                logger.info(f"(Orchestrator) Added generated title '{generated_title}' to return state.")
            else:
                logger.warning("(Orchestrator) Title generation failed or returned empty.")
        # --- End Title Generation Logic ---

    except Exception as e:
        # ... (keep existing exception handling, ensure no user message added here)
        logger.error(f"Orchestrator: Unhandled exception: {e}", exc_info=True)
        ai_reply_for_user = "[SYSTEM ERROR: Unhandled Exception]"
        new_state[STATE_STAGE] = STAGE_ERROR
        try:
            # Try to record the error response IF not already added
            if not history or history[-1].get('role') != 'model' or history[-1]['parts'][0]['text'] != ai_reply_for_user:
                 history.append({'role': 'model', 'parts': [{'text': ai_reply_for_user}]})
        except Exception as log_e: logger.error(f"Orchestrator: Failed to append error to history: {log_e}")

    # --- Final State Update & Return ---
    new_state[STATE_HISTORY] = history # Ensure history list is updated in returned state
    logger.debug(f"Orchestrator returning state: Stage={new_state.get(STATE_STAGE)}, History Len={len(new_state.get(STATE_HISTORY, []))}, Gen Title={new_state.get(STATE_GENERATED_TITLE)}, Display Syllabus={new_state.get(STATE_DISPLAY_SYLLABUS) is not None}, Transition Explainer={new_state.get(STATE_TRANSITION_EXPLAINER)}")
    return ai_reply_for_user, new_state


 